# ğŸ” C Lexical Analyzer & Syntax Validator

![Language](https://img.shields.io/badge/Language-C-blue)
![Status](https://img.shields.io/badge/Project%20Type-Compiler%20Component-brightgreen)
![License](https://img.shields.io/badge/License-MIT-yellow)
![Platform](https://img.shields.io/badge/Platform-Linux%20%7C%20Windows-lightgrey)

---

## ğŸ“˜ Overview

This project is a **C Lexical Analyzer and Syntax Validator**, designed to simulate the **lexical analysis phase of a compiler**.  
It scans a C source file, breaks it into **tokens**, identifies their types (keywords, identifiers, operators, etc.), and detects **syntax and lexical errors** such as:

- Unmatched or misplaced brackets `() {} []`
- Invalid numeric formats (binary, octal, hex, float)
- Unterminated strings and comments
- Ternary operator misplacement (`? :`)
- Invalid suffixes or malformed tokens

This project provides **line-wise color-coded output**, making it easy to debug and understand how a compiler processes C source code.

---

## ğŸ§© Features

### ğŸŸ¦ **1. Lexical Tokenization**
Detects and classifies:
- **Keywords:** `int`, `float`, `while`, `return`, etc.
- **Identifiers:** User-defined variable and function names.
- **Constants:** Integer, Floating point, Hexadecimal, Binary, and Octal numbers.
- **Operators:** Arithmetic, Relational, Logical, Bitwise, Assignment, and Conditional operators.
- **Symbols:** `{`, `}`, `()`, `[]`, `;` etc.
- **String & Character Literals:** Proper validation with error reporting.

### ğŸŸ¨ **2. Syntax Validation**
- **Bracket Checking:** Uses a stack-based mechanism to validate proper opening and closing of `()`, `{}`, and `[]`.
- **Ternary Operator Validation:** Ensures correct usage of `?` and `:` in conditional expressions.

### ğŸŸ¥ **3. Error Detection**
Reports syntax errors such as:
- Invalid suffix on constants.
- Unterminated string or character literals.
- Unterminated or nested comments.
- Extra or missing brackets.
- Invalid floating or numeric constants.
- Ternary operator misplacement.

### ğŸŸ© **4. Modular Code Structure**
The project is split into multiple files for clarity:
- `main.c` â†’ Entry point, handles file operations.
- `analyzer.c` â†’ Handles tokenization, categorization, and validation.
- `Info.c` â†’ Contains helper functions for checking keywords, operators, brackets, etc.
- `Info.h` â†’ Custom header defining macros, constants, and function prototypes.

---

## âš™ï¸ **Project Architecture**

ğŸ“ LexicalAnalyzer

â”‚

â”œâ”€â”€ ğŸ“„ main.c

â”œâ”€â”€ ğŸ“„ tokensCheck.c

â”œâ”€â”€ ğŸ“„ Functions.c

â”œâ”€â”€ ğŸ“„ Info.h

â”œâ”€â”€ ğŸ“„ test.c # (Sample input file for testing)

â””â”€â”€ ğŸ“„ README.md


Each source file has a distinct purpose:

- **main.c:** Manages command-line arguments, file reading, and function calls.
- **analyzer.c:** Core token identification logic and error management.
- **Info.c:** Utility definitions, keyword arrays, and validation functions.
- **Info.h:** Header file containing macros, function declarations, and ANSI color codes.

---

## ğŸ§  **Core Concepts Implemented**

1. **Lexical Analysis:** Breaking source code into minimal units (tokens).  
2. **Finite Automata Concept:** Used implicitly for token pattern recognition.  
3. **Stack Implementation:** Used in bracket validation for balancing parentheses.  
4. **Error Reporting System:** Displays descriptive and color-coded error messages.  
5. **File Handling:** Uses standard I/O (`fgetc`, `fseek`, `rewind`) for line-by-line scanning.

---

## ğŸ§° **Technology Stack**

| Component | Description |
|------------|--------------|
| **Language** | C |
| **Compiler** | GCC |
| **Headers Used** | `<stdio.h>`, `<string.h>`, `<stdlib.h>` |
| **Custom Header** | `Info.h` |
| **OS Compatibility** | Windows, Linux |

---

## ğŸš€ **How to Run**

 ğŸ§¾ Step 1 â€” Compile the Code
Use the GCC compiler to compile all files together:
```
gcc main.c Functions.c tokensCheck.c -o lexer
```

ğŸ§¾ Step 2 â€” Execute the Program
```

Provide a C source file as input:
./lexer example.c
```

ğŸ§¾ Step 3 â€” Output
```

The analyzer prints classified tokens and highlights syntax or lexical errors in a color-coded format such as:

Keyword : int
Identifier : main
Symbol : (
Symbol : )
Symbol : {
Numeric Constant : 10
Operator : =
String Literal : "Hello World"
ERROR :
Line No 12 : expected ':' before ';' token

```
---

## ğŸ¨ Color Legend

Color	Meaning

ğŸŸ© Green	Keyword

ğŸŸ¨ Yellow	Identifier

ğŸŸª Purple	Character Constant

ğŸŸ§ Orange	String Literal

ğŸ©µ Cyan	Operator

ğŸ©¶ Gray	Symbol

ğŸ©· Magenta	Numeric Constant

ğŸŸ¥ Red	Error / Invalid Token

---

## ğŸ§¾ Sample Output
```

Keyword : int
Identifier : main
Symbol : (
Symbol : )
Symbol : {
Identifier : a
Operator : =
Numeric Constant : 10
Symbol : ;
Identifier : printf
Symbol : (
String Literal : "Value is 10"
Symbol : )
Symbol : ;
Symbol : }

```
---

## ğŸ“Š Learning Outcomes

Through this project, I explored:

The working of lexical analyzers in compiler design.

Building a modular compiler front-end component.

How tokens are categorized and validated.

Implementing error-handling and recovery in low-level language.

The connection between automata theory and compiler construction.

---

## ğŸ§© Future Enhancements

Add symbol table generation for variables and functions.

Implement intermediate code representation (ICR).

Extend validation for nested ternary operators.

Add support for preprocessor directives like #define and #include.

Integrate with parser and semantic analyzer to form a mini compiler.

---

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome!
Feel free to fork this repo, open an issue, or submit a pull request.

---


## ğŸªª License

This project is licensed under the MIT License â€” you are free to use, modify, and distribute it for educational or personal purposes.
